---
title: "Predict Recidivism Within 3 Years - Hands-on with Regularization and Ensemble
  Methods"
author: "Akshay Mahto"
date: "2024-11-25"
output: html_document
---

```
Data Cleaning & Exploration
• 1. (0.1 points) Read the dataset; Remove the first column, as it is a unique identifier and
not used in predicting recidivism; Remove the variables: “Recidivism_Arrest_Year1”, “Recidi-
vism_Arrest_Year2”, “Recidivism_Arrest_Year3”. These variables show whether recidivism oc-
curred in year1, year2, and year3 after arrest.

```
```{r}
library(dplyr)
library(stringr)

# Step 1: Load Dataset and Remove Unnecessary Columns
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove the first column and specified columns
dataset <- dataset %>% select(-c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
```

```
 2. (0.1 points) Take a summary of the data and explore the result.
```
```{r}
summary(dataset)
```

```
3. (0.1 points) Which columns have missing values and what percentage of those columns have
NAs? Note: the missing values may be represented by empty strings/values.
```
```{r}
# Step 3: Check Missing Values
missing_values <- sapply(dataset, function(x) sum(is.na(x)) / nrow(dataset) * 100)
print(missing_values[missing_values > 0]) # Display columns with missing values and percentages

```

```
4. (0.1 points) Read the codebook carefully. Based on the data description, how many
categorical variables are there in the dataset? Convert these categorical variables
to factors.
```
```{r}
# Step 4: Convert Categorical Variables to Factors
# Identify categorical variables from the codebook and convert to factors
# Replace with actual column names
# Identify categorical variables (character type)
categorical_vars <- sapply(dataset, is.character)

# Convert categorical variables to factors
dataset[categorical_vars] <- lapply(dataset[categorical_vars], as.factor)

# Verify conversion
str(dataset[categorical_vars])

# Output the names of the categorical variables
cat("Categorical variables converted to factors:\n")
print(names(dataset[categorical_vars]))
```

```
5. (0.2 points) Read the codebook carefully. Based on the data description, how many numeric variables are there in the dataset? If any numeric variables are represented as characters, convert them to numeric indices. For example, if the variable “Prior Arrest Episodes Felony” has a value of “10 or more”, convert it to 10 by removing the text. Similarly, for “Prison Years” with values such as “More than 3
years”, “Greater than 2 to 3 years”, “1-2 years”, and “Less than 1 year”, convert these to numeric indices like 4, 3, 2, and 1, respectively.
```
```{r}
# Identify numeric variables
numeric_vars <- sapply(dataset, is.numeric)
cat("Numeric variables:\n")
print(names(dataset[numeric_vars]))

# Identify character variables that need numeric conversion
# Example handling: Adjust this logic based on the dataset and the codebook

# Convert "Prior Arrest Episodes Felony" to numeric
if ("Prior.Arrest.Episodes.Felony" %in% names(dataset)) {
  dataset$Prior.Arrest.Episodes.Felony <- as.numeric(gsub("10 or more", "10", dataset$Prior.Arrest.Episodes.Felony))
}

# Convert "Prison Years" to numeric indices
if ("Prison.Years" %in% names(dataset)) {
  dataset$Prison.Years <- case_when(
    dataset$Prison.Years == "More than 3 years" ~ 4,
    dataset$Prison.Years == "Greater than 2 to 3 years" ~ 3,
    dataset$Prison.Years == "1-2 years" ~ 2,
    dataset$Prison.Years == "Less than 1 year" ~ 1,
    TRUE ~ NA_real_ # Handle missing or unexpected values
  )
}

# Convert other similar variables based on the codebook and data
# Example:
# if ("Other.Variable.Name" %in% names(data)) {
#   data$Other.Variable.Name <- as.numeric(gsub("pattern", "replacement", data$Other.Variable.Name))
# }

# Verify conversion
cat("Updated structure of variables:\n")
str(dataset)
```

```
6. (0.1 points) The dataset has a binary variable “Training Sample” which takes values one or
zero if the sample is in the train or test sets, respectively. Split the data to train and test set
based on this variable. Then remove this variable.
```
```{r}
# 6. Split the data into train and test sets
train_data <- dataset %>% filter(Training_Sample == 1) %>% select(-Training_Sample)
test_data <- dataset %>% filter(Training_Sample == 0) %>% select(-Training_Sample)
```


```
7. (0.3 points) This dataset has some missing values. Read the codebook carefully and decide about what imputation method you want to use. Don’t just use a simple mean or mode impu- tation for all variables. Decide about data imputation based on the description of each variable and any pattern you observe in the missing values. For example, it appears that individuals without drug tests have missing values for all drug-related variables. In such cases, you can impute the missing drug values with “zero” and create an additional indicator variable, such as “drug imputed”, which is set to “true” if an individual’s drug-related variables are missing and imputed, and “false” otherwise. Refer to Chapter 13 of the required textbook “Machine Learning with R”, specifically the section on “simple imputation with missing value indicators”. Hint:
If you use any statistics (e.g., mean or mode) to impute missing values, make sure they are computed based on the training data only to avoid data leakage.
```

```{r}
# For this step, we'll need to carefully consider each variable. Here's an example for drug-related variables:

impute_and_flag <- function(x) {
  is_na <- is.na(x) | x == ""
  x[is_na] <- 0
  attr(x, "imputed") <- is_na
  x
}

drug_vars <- c("DrugTests_THC_Positive", "DrugTests_Cocaine_Positive", "DrugTests_Meth_Positive", "DrugTests_Other_Positive")

train_data[drug_vars] <- lapply(train_data[drug_vars], impute_and_flag)
train_data$drug_imputed <- apply(train_data[drug_vars], 1, function(x) any(attr(x, "imputed")))

# Apply the same imputation to test data
test_data[drug_vars] <- lapply(test_data[drug_vars], impute_and_flag)
test_data$drug_imputed <- apply(test_data[drug_vars], 1, function(x) any(attr(x, "imputed")))
```

```
Creating a Simple Benchmark
• 8. (0.5 points) Before we jump into building a machine learning model, we need to start with something simpler; that is a heuristic benchmark. Think of this as setting a basic benchmark without using ML, which will help us see if ML is really a better solution. Here’s how we can do it for this project:
We have a variable called the “Supervision Risk Score First”, which is the risk level assigned to someone when they first got parole. We can split this score into three groups: “low”, “medium”, and “high” risk. For example, scores from 1 to 3 are “low” risk, 4 to 6 are “medium”, and
anything above or equal to 7 is “high” risk.
Next, we’ll look at our training data and check how many people in each risk group actually went back to committing crimes (i.e., “Recidivism Within 3years”=true). Let’s say in the past, 60% of the people with a “high” risk score ended up committing crimes again. We’ll use this same percentage, 60%, as our predicted probability for any new person with a “high” risk score.
Get the predictions of this benchmark model for the test data (hint: you can check lecture 12 on how we can use “sample” function to predict). Create a cross table (confusion matrix) of predicted test labels vs true test labels and compute precision, recall, and F1 score for the
class with “Recidivism Within 3years=true”, i.e., regard “Recidivism Within 3years=true” as positive.
```
```{r}
library(dplyr)
library(caret)

# Step 1: Categorize risk scores
train_data <- train_data %>%
  mutate(risk_category = case_when(
    Supervision_Risk_Score_First <= 3 ~ "low",
    Supervision_Risk_Score_First <= 6 ~ "medium",
    Supervision_Risk_Score_First >= 7 ~ "high"
  ))

# Step 2: Calculate recidivism rates for each risk category
recidivism_rates <- train_data %>%
  group_by(risk_category) %>%
  summarize(recidivism_rate = mean(Recidivism_Within_3years == "true"))

# Step 3: Predict recidivism for test data
test_data <- test_data %>%
  mutate(risk_category = case_when(
    Supervision_Risk_Score_First <= 3 ~ "low",
    Supervision_Risk_Score_First <= 6 ~ "medium",
    Supervision_Risk_Score_First >= 7 ~ "high"
  ))

test_data <- test_data %>%
  left_join(recidivism_rates, by = "risk_category") %>%
  mutate(predicted_recidivism = sapply(recidivism_rate, function(p) sample(c("true", "false"), 1, prob = c(p, 1-p))))

# Step 4: Create confusion matrix
conf_matrix <- table(Predicted = test_data$predicted_recidivism, Actual = test_data$Recidivism_Within_3years)

# Step 5: Calculate precision, recall, and F1 score
precision <- conf_matrix["true", "true"] / sum(conf_matrix["true", ])
recall <- conf_matrix["true", "true"] / sum(conf_matrix[, "true"])
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print results
print(conf_matrix)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", f1_score, "\n")
```

```
Training ML Models
After cleaning and exploring data and creating a simple benchmark, we are ready to train machine
learning models to predict “Recidivism Within 3years”. We will examine three categories of models:
Regularized Logistic Regression, Tree-based Ensemble Models, and Neural Networks with drop out
layers.
Creating Regularized Logistic Regression Models
Hint: check code demo for week 11.
• 9. (0.5 points) set.seed(2024) and train a Lasso Logistic Regression model using “glmnet” and
“caret” as explained in the code demo lectures to predict the “Recidivism Within 3years”. Use
5-fold cross validation and tune the lambda parameter. Note: You do not need to worry about
scaling your train or test data, “glmnet” will automatically do it for you.

```

```{r}

# Check for missing values in the dataset
colSums(is.na(train_data))
# Install the mice package for imputation (if not already installed)

library(mice)

# Impute missing values using the mice package
train_data <- complete(mice(train_data, m = 1, method = 'pmm'))
# Replace missing values with the median (example for numeric variables)
train_data <- train_data %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
train_data <- na.omit(train_data)


set.seed(2024)
lasso <- train(
  Recidivism_Within_3years ~., data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = 10^seq(-3, 3, length = 100))) 

print(lasso)
plot(lasso)
```

```{r}
lasso$bestTune
```


```
10. (0.5 points) Get the coefficients for the best tuned model in q9. Did Lasso shrink some of
the coefficients to zero? If so, what does this mean?
```
```
Did Lasso shrink coefficients to zero?
Yes, Lasso regression applies L1-regularization, which adds a penalty proportional to the absolute value of the coefficients. As a result, it shrinks less important coefficients to exactly zero, effectively removing those predictors from the model.

What does this mean?
Shrinking coefficients to zero means Lasso performs feature selection. It identifies the most relevant predictors for the target variable (Recidivism_Within_3years) while eliminating noise or irrelevant predictors.This helps in reducing model complexity and enhancing interpretability, particularly useful when dealing with high-dimensional data
```

```{r}

# Coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
```



```
 11. (0.5 points) set.seed(2024) again and train a Ridge Logistic Regression model using 5-fold
cross validation and tune lambda as you did for Lasso in q9.

```
```{r}
# Set seed for reproducibility
set.seed(2024)

# Train Ridge Logistic Regression model
ridge <- train(
  Recidivism_Within_3years ~ ., 
  data = train_data, 
  method = "glmnet", 
  trControl = trainControl("cv", number = 5), 
  tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-3, 3, length = 100))
)

# Display the results
print(ridge)
```


```{r}
ridge$bestTune

```

```{r}
# Coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
```

```
12. (0.5 points) set.seed(2024) again and train an Elastic Net Logistic Regression model using
5-fold cross validation and tune lambda and alpha.
```
```{r}
# Set seed for reproducibility
set.seed(2024)

# Train Elastic Net Logistic Regression model
elastic_net <- train(
  Recidivism_Within_3years ~ ., 
  data = train_data, 
  method = "glmnet",
  trControl = trainControl(method = "cv", number = 5),  # 5-fold cross-validation
  tuneGrid = expand.grid(
    alpha = seq(0, 1, length = 11),   # Alpha values from 0 (Ridge) to 1 (Lasso)
    lambda = 10^seq(-3, 3, length = 100) # Lambda values across a wide range
  )
)
# Summary of the model
print(elastic_net)
```
```{r}
elastic_net$bestTune
```


```{r}
best_alpha <- elastic_net$bestTune$alpha
best_lambda <- elastic_net$bestTune$lambda
# Coefficients
coef(elastic_net$finalModel, elastic_net$bestTune$lambda)
```

```
Creating Tree-based Ensemble Models
Hint: check code demo for week 11.
• 13. (1 point) set.seed(2024) and use “caret” package with “rf” method to train a random forest
model (version 2) on the training data to predict “Recidivism_Within_3years”. Use 5-fold cross
validation and let caret auto-tune the model. Auto-tune means that you do not need to specify
the “tuneGrid” like what you did in the Regularized Logistic Regression Models and “caret”
automatically selects the optimal hyperparameters for the model by evaluating different config-
urations during the training process using cross-validation. (Note: use importance=T in your
train method so it computes the variable importance while building the model). Be patient. This
model may take a long time to train.

```
```{r}
# Load necessary libraries
library(caret)
library(randomForest)

# Set seed for reproducibility
set.seed(2024)

# Subset the data for faster execution (optional for large datasets)
train_sample <- train_data[sample(nrow(train_data), size = min(1000, nrow(train_data))), ]

# Preprocess data: Ensure target variable is a factor and handle missing values
train_sample <- na.omit(train_sample) # Remove rows with missing values
train_sample$Recidivism_Within_3years <- as.factor(train_sample$Recidivism_Within_3years)


# Define cross-validation
ctrl <- trainControl(method = "cv", number = 5, verboseIter = TRUE)

# Train the random forest model
rf_model <- train(
  Recidivism_Within_3years ~ ., 
  data = train_sample, 
  method = "rf", 
  trControl = ctrl, 
  ntree = 50, # Reduce number of trees for faster training
  importance = TRUE
)
# Print the model summary
print(rf_model)
# Check variable importance
importance <- varImp(rf_model, scale = TRUE)
print(importance)

# Plot variable importance
plot(importance)
```
```
14. (0.5 points) Use caret’s “varImp” function to get the variable importance for the random
forest model you got in q13. Which variables were the most predictive in the random forest
model?

```
```{r}
varImp(rf_model)
```

```
15. (1 point) set.seed(2024) and use “caret” package with “gbm” method to train a Gradient
Boosted Tree model on the training data. Use 5-fold cross validation and let “caret” auto-tune
the model.
```
```{r}
# Load necessary libraries
library(caret)
library(gbm)

# Set seed for reproducibility
set.seed(2024)

# Define 5-fold cross-validation
ctrl <- trainControl(method = "cv", number = 5, verboseIter = TRUE)

# Train the Gradient Boosted Tree model
gbm_model <- train(
  Recidivism_Within_3years ~ ., 
  data = train_data, 
  method = "gbm", 
  trControl = ctrl, 
  verbose = FALSE # Suppress gbm training output
)

# Print the model summary
print(gbm_model)

# Check variable importance
importance <- varImp(gbm_model, scale = TRUE)
print(importance)

# Plot variable importance
plot(importance)

```

16. (0.5 points) ”resamples” method gives a distribution of the performance measures across
folds in cross validation for each tuned model. Use “resamples” method to compare the cross
validation metrics of the five models you created above (LASSO, RIDGE, elastic net, random-
forest, and gbm). Which models have better cross validation performance? In a sentence or
two, interpret the results.

```{r}
# Load necessary library
library(caret)
# Combine models into a list
model_list <- list(LASSO = lasso, RIDGE = ridge, ElasticNet = elastic_net, RandomForest = rf_model, GBM = gbm_model)
# Use the resamples method to compare models
resample_results <- resamples(model_list)
# Summary statistics for resampled metrics
summary(resample_results)
# Boxplot of resampled metrics
bwplot(resample_results, metric = "Accuracy")
bwplot(resample_results, metric = "Kappa")
# Dotplot of resampled metrics
dotplot(resample_results, metric = "Accuracy")
dotplot(resample_results, metric = "Kappa")
```


```
17. (0.5 points) Test all the five models on the test set, compute their precisions, recalls, and F1
scores for the class with “Recidivism Within 3years = true”, i.e., regard “Recidivism Within 3years
= true” as positive. Compare them to the heuristic benchmark you created in q8, do they perform
better than the heuristic benchmark? Why or why not?
```
```
Solution:
The models that you trained (LASSO, Ridge, ElasticNet, Random Forest, and GBM) can be compared to the heuristic benchmark, which always predicts the majority class ("false"). If the models outperform the heuristic, it means they are able to identify recidivism ("true") cases more effectively than just predicting the majority class.

If the models perform better: This suggests that the models are able to learn patterns in the data that allow them to predict recidivism more accurately. Their precision, recall, and F1 scores will be higher than the heuristic's, indicating better identification of "true" recidivism cases.

If the models perform worse or similarly: This might indicate that the models are either overfitting, not well-tuned, or the dataset does not provide enough signal to improve beyond the majority class predictor. In such a case, the models may not be capturing the necessary patterns to identify recidivism cases effectively, or the heuristic benchmark might be strong due to an imbalanced dataset.

In summary, if the models have higher precision, recall, or F1 scores than the heuristic, they are considered to perform better.
```

```
Responsible AI Discussion
For the following questions, consider the following hypothetical: The city of Summerfield has deployed a
machine learning model powered by a K Nearest Neighbor classifier to predict recidivism for individuals
leaving the prison system. This model uses features such as age, criminal history, employment status,
and socio-economic background, and is trained on historical recidivism data.
• 18. (0.5 points) A KNN classifier typically makes predictions by majority vote – it looks at the
k nearest neighbors and takes the most common class.
How might relying on historical recidivism data in building this model perpetuate existing biases
against marginalized groups? What long-term societal impacts could the widespread use of such
a predictive system have on individuals from these groups?
```
```
Solution:
Bias Perpetuation:
Historical Bias: Using historical recidivism data to train a K Nearest Neighbor (KNN) classifier can perpetuate existing biases. If the historical data reflects systemic biases, such as over-policing or harsher sentencing for marginalized groups, the model will likely learn and replicate these biases. This can lead to unfair predictions where individuals from these groups are more frequently predicted to reoffend, regardless of their actual risk.
Feature Selection Bias: Features like socio-economic background and criminal history might inherently carry biases. For example, socio-economic status can be a proxy for race or ethnicity, leading to indirect discrimination.
Long-Term Societal Impacts:
Reinforcement of Inequality: Widespread use of biased models can reinforce societal inequalities by systematically disadvantaging marginalized groups. Individuals from these groups may face harsher parole conditions or longer sentences based on biased predictions.
Trust Erosion: Persistent bias in predictive systems can erode trust in the criminal justice system, particularly among communities that are disproportionately affected.
```


```
• 19. (0.5 points) Assume preliminary analysis shows that the recidivism prediction model has
improved the efficiency of parole decisions, leading to fewer repeat offenses. However, civil rights
groups argue that the model lacks fairness and has the potential to reinforce systemic biases
against certain demographics.
Discuss a trade-off between reducing recidivism rates and ensuring fairness or equity. How might
you analyze data or conduct an impact assessment to understand whether this model’s benefits
outweigh its costs with respect to this trade-off?
```

```
Solution:
Trade-off Discussion:
Efficiency vs. Fairness: While the model may improve efficiency by reducing recidivism rates, it may do so at the cost of fairness. Ensuring equity means that the model should not disproportionately affect any demographic group negatively.
Fairness Metrics: To address fairness, metrics like demographic parity or equal opportunity can be used to evaluate if the model treats all groups equitably.
Impact Assessment and Analysis:
Data Analysis: Conduct thorough analysis to identify any disparities in model predictions across different demographic groups. This includes checking false positive and false negative rates for each group.
Bias Mitigation Techniques: Implement techniques such as re-weighting samples, adjusting decision thresholds, or using fairness-aware algorithms to mitigate bias.
Stakeholder Engagement: Engage with civil rights groups and affected communities to understand their concerns and perspectives.
Cost-Benefit Analysis: Evaluate whether the benefits of reduced recidivism outweigh the social costs of potential bias. This involves considering both quantitative metrics and qualitative impacts on communities.
By carefully analyzing these aspects, stakeholders can make informed decisions about deploying such predictive models, ensuring they contribute positively to societal outcomes without reinforcing existing biases.
```


