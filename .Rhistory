tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-3, 3, length = 100))
)
# Set seed for reproducibility
set.seed(2024)
# Train Ridge Logistic Regression model
ridge <- train(
Recidivism_Within_3years ~ .,
data = train_data,
method = "glmnet",
trControl = trainControl("cv", number = 5),
tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-3, 3, length = 100))
)
# Display the results
print(ridge)
ridge$bestTune
coef(ridge$finalModel, ridge$bestTune$lambda)
# Set seed for reproducibility
set.seed(2024)
# Train Ridge Logistic Regression model
ridge <- train(
Recidivism_Within_3years ~ .,
data = train_data,
method = "glmnet",
trControl = trainControl("cv", number = 5),
tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-3, 3, length = 100))
)
# Display the results
print(ridge)
ridge$bestTune
coef(ridge$finalModel, ridge$bestTune$lambda)
# Set seed for reproducibility
set.seed(2024)
# Train Ridge Logistic Regression model
ridge <- train(
Recidivism_Within_3years ~ .,
data = train_data,
method = "glmnet",
trControl = trainControl("cv", number = 5),
tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-3, 3, length = 100))
)
# Display the results
print(ridge)
ridge$bestTune
coef(ridge$finalModel, ridge$bestTune$lambda)
# Set seed for reproducibility
set.seed(2024)
# Train Elastic Net Logistic Regression model
elastic_net <- train(
Recidivism_Within_3years ~ .,
data = train_data,
method = "glmnet",
trControl = trainControl(method = "cv", number = 5),  # 5-fold cross-validation
tuneGrid = expand.grid(
alpha = seq(0, 1, length = 11),   # Alpha values from 0 (Ridge) to 1 (Lasso)
lambda = 10^seq(-3, 3, length = 100) # Lambda values across a wide range
)
)
# Summary of the model
print(elastic_net)
elastic_net$bestTune
best_alpha <- elastic_net$bestTune$alpha
best_lambda <- elastic_net$bestTune$lambda
# Coefficients
coef(elastic_net$finalModel, elastic_net$bestTune$lambda)
#13
library(caret)
set.seed(2024)
rf_model <- train(Recidivism_Within_3years ~ .,
data = train_data, method = "rf", metric = "Kappa",
trControl= trainControl(method = "cv", number = 5), importance = T)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
library(dplyr)
library(stringr)
# Step 1: Load Dataset and Remove Unnecessary Columns
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove the first column and specified columns
dataset <- dataset %>% select(-c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
summary(dataset)
# Step 3: Check Missing Values
missing_values <- sapply(dataset, function(x) sum(is.na(x)) / nrow(dataset) * 100)
print(missing_values[missing_values > 0]) # Display columns with missing values and percentages
# Step 4: Convert Categorical Variables to Factors
# Identify categorical variables from the codebook and convert to factors
# Replace with actual column names
# Identify categorical variables (character type)
categorical_vars <- sapply(dataset, is.character)
# Convert categorical variables to factors
dataset[categorical_vars] <- lapply(dataset[categorical_vars], as.factor)
# Verify conversion
str(dataset[categorical_vars])
# Output the names of the categorical variables
cat("Categorical variables converted to factors:\n")
print(names(dataset[categorical_vars]))
# Identify numeric variables
numeric_vars <- sapply(dataset, is.numeric)
cat("Numeric variables:\n")
print(names(dataset[numeric_vars]))
# Identify character variables that need numeric conversion
# Example handling: Adjust this logic based on the dataset and the codebook
# Convert "Prior Arrest Episodes Felony" to numeric
if ("Prior.Arrest.Episodes.Felony" %in% names(dataset)) {
dataset$Prior.Arrest.Episodes.Felony <- as.numeric(gsub("10 or more", "10", dataset$Prior.Arrest.Episodes.Felony))
}
# Convert "Prison Years" to numeric indices
if ("Prison.Years" %in% names(dataset)) {
dataset$Prison.Years <- case_when(
dataset$Prison.Years == "More than 3 years" ~ 4,
dataset$Prison.Years == "Greater than 2 to 3 years" ~ 3,
dataset$Prison.Years == "1-2 years" ~ 2,
dataset$Prison.Years == "Less than 1 year" ~ 1,
TRUE ~ NA_real_ # Handle missing or unexpected values
)
}
# Convert other similar variables based on the codebook and data
# Example:
# if ("Other.Variable.Name" %in% names(data)) {
#   data$Other.Variable.Name <- as.numeric(gsub("pattern", "replacement", data$Other.Variable.Name))
# }
# Verify conversion
cat("Updated structure of variables:\n")
str(dataset)
View(dataset)
# 6. Split the data into train and test sets
train_data <- dataset %>% filter(Training_Sample == 1) %>% select(-Training_Sample)
test_data <- dataset %>% filter(Training_Sample == 0) %>% select(-Training_Sample)
# For this step, we'll need to carefully consider each variable. Here's an example for drug-related variables:
impute_and_flag <- function(x) {
is_na <- is.na(x) | x == ""
x[is_na] <- 0
attr(x, "imputed") <- is_na
x
}
drug_vars <- c("DrugTests_THC_Positive", "DrugTests_Cocaine_Positive", "DrugTests_Meth_Positive", "DrugTests_Other_Positive")
train_data[drug_vars] <- lapply(train_data[drug_vars], impute_and_flag)
train_data$drug_imputed <- apply(train_data[drug_vars], 1, function(x) any(attr(x, "imputed")))
# Apply the same imputation to test data
test_data[drug_vars] <- lapply(test_data[drug_vars], impute_and_flag)
test_data$drug_imputed <- apply(test_data[drug_vars], 1, function(x) any(attr(x, "imputed")))
library(dplyr)
library(caret)
# Step 1: Categorize risk scores
train_data <- train_data %>%
mutate(risk_category = case_when(
Supervision_Risk_Score_First <= 3 ~ "low",
Supervision_Risk_Score_First <= 6 ~ "medium",
Supervision_Risk_Score_First >= 7 ~ "high"
))
# Step 2: Calculate recidivism rates for each risk category
recidivism_rates <- train_data %>%
group_by(risk_category) %>%
summarize(recidivism_rate = mean(Recidivism_Within_3years == "true"))
# Step 3: Predict recidivism for test data
test_data <- test_data %>%
mutate(risk_category = case_when(
Supervision_Risk_Score_First <= 3 ~ "low",
Supervision_Risk_Score_First <= 6 ~ "medium",
Supervision_Risk_Score_First >= 7 ~ "high"
))
test_data <- test_data %>%
left_join(recidivism_rates, by = "risk_category") %>%
mutate(predicted_recidivism = sapply(recidivism_rate, function(p) sample(c("true", "false"), 1, prob = c(p, 1-p))))
# Step 4: Create confusion matrix
conf_matrix <- table(Predicted = test_data$predicted_recidivism, Actual = test_data$Recidivism_Within_3years)
# Step 5: Calculate precision, recall, and F1 score
precision <- conf_matrix["true", "true"] / sum(conf_matrix["true", ])
recall <- conf_matrix["true", "true"] / sum(conf_matrix[, "true"])
f1_score <- 2 * (precision * recall) / (precision + recall)
# Print results
print(conf_matrix)
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", f1_score, "\n")
# Check for missing values in the dataset
colSums(is.na(train_data))
# Install the mice package for imputation (if not already installed)
library(mice)
# Impute missing values using the mice package
train_data <- complete(mice(train_data, m = 1, method = 'pmm'))
# Replace missing values with the median (example for numeric variables)
train_data <- train_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
train_data <- na.omit(train_data)
set.seed(2024)
lasso <- train(
Recidivism_Within_3years ~., data = train_data, method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneGrid = expand.grid(alpha = 1, lambda = 10^seq(-3, 3, length = 100)))
print(lasso)
plot(lasso)
lasso$bestTune
# Coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Set seed for reproducibility
set.seed(2024)
# Train Ridge Logistic Regression model
ridge <- train(
Recidivism_Within_3years ~ .,
data = train_data,
method = "glmnet",
trControl = trainControl("cv", number = 5),
tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(-3, 3, length = 100))
)
# Display the results
print(ridge)
ridge$bestTune
# Coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
# Set seed for reproducibility
set.seed(2024)
# Train Elastic Net Logistic Regression model
elastic_net <- train(
Recidivism_Within_3years ~ .,
data = train_data,
method = "glmnet",
trControl = trainControl(method = "cv", number = 5),  # 5-fold cross-validation
tuneGrid = expand.grid(
alpha = seq(0, 1, length = 11),   # Alpha values from 0 (Ridge) to 1 (Lasso)
lambda = 10^seq(-3, 3, length = 100) # Lambda values across a wide range
)
)
# Summary of the model
print(elastic_net)
elastic_net$bestTune
best_alpha <- elastic_net$bestTune$alpha
best_lambda <- elastic_net$bestTune$lambda
# Coefficients
coef(elastic_net$finalModel, elastic_net$bestTune$lambda)
library(caret)
set.seed(2024)
rf_model <- train(Recidivism_Within_3years ~ .,
data = train_data, method = "rf", metric = "Kappa",
trControl= trainControl(method = "cv", number = 5), importance = T)
library(caret)
ctrl <- trainControl(method = "cv", number = 10)
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
set.seed(2024)
m_rf <- train(default ~ ., data = credit_train, method = "rf", trControl = ctrl, tuneGrid = grid_rf) # take some time
credit <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
credit_train.index=createDataPartition(credit$default,p = 0.8, list = FALSE)
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism.Arrest.Year1, Recidivism.Arrest.Year2, Recidivism.Arrest.Year3))
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 10)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(
Recidivism_Within_3years ~ .,
data = train_data,
method = "rf",
trControl = ctrl,
tuneGrid = grid_rf
)
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 10)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(default ~ ., data = credit_train, method = "rf", trControl = ctrl, tuneGrid = grid_rf) # take some time
m_rf
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(default ~ ., data = credit_train, method = "rf", trControl = ctrl, tuneGrid = grid_rf) # take some time
m_rf
varImp(m_rf)
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(Recidivism_Within_3years ~ ., data = train_data, method = "rf", trControl = ctrl, tuneGrid = grid_rf) # take some time
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(default ~ ., data = train_data, method = "rf", trControl = ctrl, tuneGrid = grid_rf) # take some time
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(dataset ~ ., data = train_data, method = "rf", trControl = ctrl, tuneGrid = grid_rf) # take some time
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(default ~ ., data = credit_train, method = "rf", trControl = ctrl, tuneGrid = grid_rf) # take some time
m_rf
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(default ~ ., data = credit_train, method = "rf", metric = "kappa", trControl = ctrl, tuneGrid = grid_rf, importance = T) # take some time
m_rf
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
dataset_train.index <- createDataPartition(dataset$default, p = 0.8, list = FALSE)
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$default, p = 0.8, list = FALSE)
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
credit <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/credit.csv")
credit_train.index=createDataPartition(credit$default,p = 0.8, list = FALSE)
credit_train=credit[credit_train.index, ]
credit_test <- credit[-credit_train.index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(default ~ ., data = credit_train, method = "rf", metric = "kappa", trControl = ctrl, tuneGrid = grid_rf, importance = T) # take some time
m_rf
# Load necessary libraries
library(caret)
# Load the dataset
dataset <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/NIJ_s_Recidivism_Challenge_Full_Dataset_20240826.csv")
# Remove irrelevant columns
dataset <- subset(dataset, select = -c(1, Recidivism_Arrest_Year1, Recidivism_Arrest_Year2, Recidivism_Arrest_Year3))
# Partition dataset (80% train, 20% test)
set.seed(2024)
train_index <- createDataPartition(dataset$Recidivism_Within_3years, p = 0.8, list = FALSE)
# Create train and test sets
train_data <- dataset[train_index, ]
test_data <- dataset[-train_index, ]
credit <- read.csv("/Users/apple/Desktop/projects/ML Assignment 5/credit.csv")
credit_train.index=createDataPartition(credit$default,p = 0.8, list = FALSE)
credit_train=credit[credit_train.index, ]
credit_test <- credit[-credit_train.index, ]
# Convert target variable to factor
test_data$Recidivism_Within_3years <- factor(test_data$Recidivism_Within_3years)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Define grid for tuning
grid_rf <- expand.grid(mtry = c(2, 4, 8, 16))
# Train Random Forest model
set.seed(2024)
m_rf <- train(default ~ ., data = credit_train, method = "rf", trControl = ctrl, tuneGrid = grid_rf, importance = T) # take some time
m_rf
# Check variable importance
varImp(rf_model)
varImp(m_rf)
# Load necessary library
library(caret)
# Define control for cross-validation
ctrl <- trainControl(method = "cv", number = 5)
# Train the GBM model
set.seed(2024)
gbm_model <- train(
Recidivism.Within.3years ~ .,
data = train_data,
method = "gbm",
trControl = ctrl,
verbose = FALSE
)
